@article{abdaoui2020load,
  title={Load What You Need: Smaller Versions of Multilingual {BERT}},
  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr{\'e}goire},
  journal={arXiv preprint arXiv:2010.05609},
  year={2020}
}

@inproceedings{nair2023blade,
  title={{BLADE}: Bilingually Pruned Language-Specific Adaptations for Cross-Language Information Retrieval},
  author={Nair, Sthitapragyan and Khattab, Omar and Boytsov, Leonid and Oard, Douglas W},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1415--1425},
  year={2023}
}

@article{dorkin2025estonian,
  title={Adapting Language Models for Estonian: A Case Study in Language-Specific Optimizations},
  author={Dorkin, K and Adamson, P and Tammur, L},
  journal={arXiv preprint arXiv:2025.xxxxx},
  year={2025}
}

@article{derijck2024words,
  title={Words That Matter: Vocabulary Pruning in {ModernBERT}},
  author={De Rijck, Wout and D'Oosterlinck, Karel and Demeester, Thomas and Develder, Chris},
  journal={In preparation},
  year={2024}
}

@inproceedings{shen2022textpruner,
  title={{TextPruner}: A Model Pruning Toolkit for Pre-trained Language Models},
  author={Shen, Ziqing and Cui, Yiming and Cheng, Zhiyang and Zhang, Hao and Feng, Dongyan},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={17--27},
  year={2022},
  publisher={Association for Computational Linguistics}
}

@inproceedings{formal2023spladex,
  title={{SPLADE-X}: Expanding Retrieval-Oriented Language Representations Beyond English},
  author={Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2022--2032},
  year={2023}
}

@article{modernbert2023,
  title={{ModernBERT}: Pushing the Limits of Efficient Transformer-Based Language Models},
  author={ModernBERT Team},
  journal={arXiv preprint arXiv:2023.xxxxx},
  year={2023}
}

@inproceedings{losparse2023,
  title={{LoSparse}: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation},
  author={Yang, Yikang and Zhong, Tao and Zeng, Wentao and Shao, Zhixuan and Jiang, Xin and Feng, Yanzhe and others},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={39872--39883},
  year={2023}
}

@article{wang2018glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{tfidf1972,
  title={A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author={Sp{\"a}rck Jones, Karen},
  journal={Journal of Documentation},
  volume={28},
  number={1},
  pages={11--21},
  year={1972},
  publisher={MCB UP Ltd}
}

@article{kmeans1967,
  title={Some Methods for Classification and Analysis of Multivariate Observations},
  author={MacQueen, James},
  journal={Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability},
  volume={1},
  number={14},
  pages={281--297},
  year={1967}
}

@article{bousquet2023pema,
  title={{PEMA}: Partial Embedding Matrix Adaptation for Efficient Language Model Fine-tuning},
  author={Bousquet, Pierre-Francois and Rajovic, Nikola and Jordan, Michael},
  journal={arXiv preprint arXiv:2306.01355},
  year={2023}
}

@inproceedings{wang2023lighttoken,
  title={{LightToken}: Lightweight Token Embedding for Efficient Natural Language Understanding},
  author={Wang, Tao and Zhou, Lingming and Zhao, Yue and Sarkar, Chandan and Sun, Mingyang and Pan, Lin and Du, Xiaodan and Yang, Ruoyu and Luo, Huazhong},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  pages={4412--4425},
  year={2023}
}

@article{michel2019heads,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={arXiv preprint arXiv:1905.10650},
  year={2019}
}

@inproceedings{voita2019heads,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5797--5808},
  year={2019},
  organization={Association for Computational Linguistics}
}

@article{sanh2020movement,
  title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20378--20389},
  year={2020}
}

@inproceedings{fan2020layerdrop,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  booktitle={International Conference on Learning Representations},
  year={2020}
} 