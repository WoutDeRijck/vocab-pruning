@article{wan2023efficient,
  title={Efficient Large Language Models: A Survey},
  author={Wan, Zhongwei and Wang, Xin and others},
  journal={arXiv preprint arXiv:2312.03863},
  year={2023},
  url={https://arxiv.org/abs/2312.03863},
  note={Accepted to Transactions on Machine Learning Research (TMLR), May 2024}
}

@article{abdaoui2020load,
  title={Load What You Need: {BERT} Optimized for Speed},
  author={Abdaoui, Amine and Fehrentz, Arnaud and others},
  journal={arXiv preprint arXiv:2007.02450},
  year={2020}
}

@inproceedings{nair2023blade,
  title={{BLADE}: Combining Vocabulary Pruning and Intermediate Pretraining for Scaleable Neural CLIR},
  author={Nair, Suraj and Maillard, Jean and others},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},
  year={2023}
}

@article{dorkin2025estonian,
  title={Vocabulary Pruning for Estonian {BERT}-based {NER} Models},
  author={Dorkin, Evelin and others},
  journal={Transactions of the Association for Computational Linguistics (TACL)},
  year={2025},
  note={Forthcoming}
}

@inproceedings{shen2022textpruner,
  title={{TextPruner}: A Unified Framework for Pruning Token-Level and Feature-Level Redundancy in Text},
  author={Shen, Yelong and others},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2022}
}

@inproceedings{formal2023spladex,
  title={{SPLADE-X}: Expanding Retrieval-Oriented Language Representations Beyond English},
  author={Formal, Thibault and Lassance, Carlos and others},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},
  pages={2022--2032},
  year={2023}
}

@inproceedings{losparse2023,
  title={{LoSparse}: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation},
  author={Yang, Yikang and Zhong, Tao and others},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  pages={39872--39883},
  year={2023}
}

@article{wang2018glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and others},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{tfidf1972,
  title={A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author={Sp{\"a}rck Jones, Karen},
  journal={Journal of Documentation},
  volume={28},
  number={1},
  pages={11--21},
  year={1972},
  publisher={MCB UP Ltd}
}

@article{kmeans1967,
  title={Some Methods for Classification and Analysis of Multivariate Observations},
  author={MacQueen, James},
  journal={Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability},
  volume={1},
  number={14},
  pages={281--297},
  year={1967}
}

@article{bousquet2023pema,
  title={{PEMA}: Efficient Fine-tuning by Partial Embedding Matrix Adaptation},
  author={Bousquet, Timoth√©e and others},
  journal={arXiv preprint arXiv:2303.00868},
  year={2023}
}

@inproceedings{wang2023lighttoken,
  title={{LightToken}: Structured Embedding Compression for Transformer Inference},
  author={Wang, Yifei and others},
  booktitle={Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  year={2023}
}

@inproceedings{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and others},
  booktitle={Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@inproceedings{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and others},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019}
}

@inproceedings{sanh2020movement,
  title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  author={Sanh, Victor and Xu, Albert Webson and others},
  booktitle={Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{fan2020layerdrop,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Fan, Angela and Grave, Edouard and others},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inproceedings{li2023losparse,
  title={LoSparse: Structured Pruning via Low-Rank and Sparse Decomposition},
  author={Li, Zixuan and Ding, Meiqi and others},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{namburi2023llm,
  title={{LLM} in a Flash: Efficient Large Language Model Inference with Limited Memory},
  author={Namburi, Raghuveer and Rotem, Nadav and others},
  journal={arXiv preprint arXiv:2309.16104},
  year={2023},
  url={https://arxiv.org/abs/2309.16104}
}

@misc{Hinton2015Distillation,
  title = {Distilling the Knowledge in a Neural Network},
  author = {Hinton, Geoffrey and Vinyals, Oriol and others},
  year = {2015},
  eprint = {1503.02531},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  note = {Technical report},
  url = {https://arxiv.org/abs/1503.02531}
}

@misc{Hu2021LoRA,
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  author = {Hu, Edward J. and Shen, Yelong and others},
  year = {2021},
  eprint = {2106.09685},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2106.09685}
}

@InProceedings{pmlr-v235-li24bi,
  title = {LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models},
  author = {Li, Guangyan and Tang, Yongqiang and others},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  pages = {28657--28672},
  year = {2024},
  volume = {235},
  series = {Proceedings of Machine Learning Research},
  month = {23--29 Jul},
}

@misc{huang2023awesome,
  author = {Huang, Owen},
  title = {Awesome LLM Compression},
  year = {2023},
  howpublished = {\url{https://github.com/HuangOwen/Awesome-LLM-Compression}},
  note = {Accessed: 2025-05-19}
}

@article{ren2025llmcompression,
  title={Large Language Model Compression with Global Rank and Sparsity Optimization},
  author={Ren, Yujia and Zhu, Yifan},
  journal={arXiv preprint arXiv:2505.03801},
  year={2025},
  url={https://arxiv.org/abs/2505.03801}
}

@inproceedings{mikolov2013efficient,
  title     = {Efficient Estimation of Word Representations in Vector Space},
  author    = {Tomas Mikolov and Kai Chen and others},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year      = {2013},
  url       = {https://arxiv.org/abs/1301.3781}
}

@article{li2024enhancing,
  title={Enhancing performance of transformer-based models in natural language understanding through word importance embedding},
  author={Li, Yuhao and Xia, Tao and others},
  journal={Knowledge-Based Systems},
  volume={287},
  pages={111443},
  year={2024},
  url={https://www.sciencedirect.com/science/article/pii/S0950705124010384}
}

@inproceedings{guo2024attention,
  title     = {Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters},
  author    = {Zhiyu Guo and Hidetaka Kamigaito and others},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {21158--21166},
  year      = {2024},
  month     = {November},
  url       = {https://aclanthology.org/2024.emnlp-main.1178/}
}

@article{wang2019dbscan,
  title={DBSCAN: Optimal Rates For Density-Based Cluster Estimation},
  author={Wang and Daren others},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={170},
  pages={1--50},
  year={2019},
  url={https://www.jmlr.org/papers/v20/18-470.html}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and others},
  booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1631--1642},
  year={2013}
}

@article{warstadt2019neural,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and others},
  journal={Transactions of the Association for Computational Linguistics (TACL)},
  volume={7},
  pages={625--641},
  year={2019}
}

@inproceedings{dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, William B and others},
  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},
  year={2005}
}

@inproceedings{cer2017semeval,
  title={SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation},
  author={Cer, Daniel and others},
  booktitle={Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
  pages={1--14},
  year={2017}
}

@misc{qqp,
  title={Quora Question Pairs},
  howpublished={\url{https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs}},
  note={Accessed: 2025-06-01}
}

@inproceedings{williams2018broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and others},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  pages={1112--1122},
  year={2018}
}

@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and others},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2383--2392},
  year={2016}
}

@inproceedings{dagan2005pascal,
  title={The PASCAL Recognising Textual Entailment Challenge},
  author={Dagan, Ido and others},
  booktitle={Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment},
  year={2005}
}

@article{yuan2025efficientllm,
  title={EfficientLLM: Efficiency in Large Language Models},
  author={Yuan, Zhengqing and Sun, Weixiang and others},
  journal={arXiv preprint arXiv:2505.13840},
  year={2025}
}

@misc{warner2024modernbert,
  title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},
  author={Warner, Benjamin and Chaffin, Antoine and others},
  year={2024},
  eprint={2412.13663},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2412.13663}
}

@article{yang2022taskspecific,
  title={Task-specific Compression for Multi-task Language Models using Attribution-based Pruning},
  author={Yang, Nakyeong and others},
  journal={arXiv preprint arXiv:2205.04157},
  year={2022}
}

@inproceedings{kim2022learned,
  title={Learned Token Pruning for Transformers},
  author={Kim, Sehoon and others},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)},
  pages={784--794},
  year={2022},
  doi={10.1145/3534678.3539260},
  url={https://doi.org/10.1145/3534678.3539260}
}

@inproceedings{goyal2020powerbert,
  title={PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination},
  author={Goyal, Saurabh and others},
  booktitle={Proceedings of the 37th International Conference on Machine Learning (ICML)},
  volume={119},
  pages={3690--3699},
  year={2020},
  url={https://proceedings.mlr.press/v119/goyal20a.html}
}

@inproceedings{chen2021earlybert,
  title={EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets},
  author={Chen, Xiaohan and others},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP)},
  pages={2195--2207},
  year={2021},
  doi={10.18653/v1/2021.acl-long.171},
  url={https://aclanthology.org/2021.acl-long.171}
}

@article{park2024comprehensive,
  title={A Comprehensive Survey of Compression Algorithms for Language Models},
  author={Park, Seungcheol and others},
  journal={arXiv preprint arXiv:2401.15347},
  year={2024}
}

@article{cheng2017survey,
  title={A Survey of Model Compression and Acceleration for Deep Neural Networks},
  author={Cheng, Yu and others},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017}
}

@article{mishra2020survey,
  title={A Survey on Deep Neural Network Compression: Challenges, Overview, and Solutions},
  author={Mishra, Rahul and others},
  journal={arXiv preprint arXiv:2010.03954},
  year={2020}
}

@misc{xailient2021compression,
  title={4 Popular Model Compression Techniques Explained},
  author={{Xailient}},
  year={2021},
  howpublished={\url{https://xailient.com/blog/4-popular-model-compression-techniques-explained/}}
}

@article{muralidharan2024compact,
  title={Compact Language Models via Pruning and Knowledge Distillation},
  author={Muralidharan, Saurav and others},
  journal={arXiv preprint arXiv:2407.14679},
  year={2024},
  url={https://arxiv.org/abs/2407.14679}
}

@article{han2015deep,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author={Han, Song and others},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{choukroun2019low,
  title={Low-bit Quantization of Neural Networks for Efficient Inference},
  author={Choukroun, Yoni and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  year={2019}
}

@article{cheng2023survey,
  title={A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations},
  author={Cheng, Hongrong and others},
  journal={arXiv preprint arXiv:2308.06767},
  year={2023},
  url={https://arxiv.org/abs/2308.06767}
}

@article{gholami2021survey,
  title={A Survey of Quantization Methods for Efficient Neural Network Inference},
  author={Gholami, Amir and others},
  journal={arXiv preprint arXiv:2103.13630},
  year={2021},
  url={https://arxiv.org/abs/2103.13630}
}