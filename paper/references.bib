@article{wan2023efficient,
  title={Efficient Large Language Models: A Survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and Chowdhury, Mosharaf and Zhang, Mi},
  journal={arXiv preprint arXiv:2312.03863},
  year={2023},
  url={https://arxiv.org/abs/2312.03863},
  note={Accepted to Transactions on Machine Learning Research (TMLR), May 2024}
}

@article{abdaoui2020load,
  title={Load What You Need: {BERT} Optimized for Speed},
  author={Abdaoui, Amine and Fehrentz, Arnaud and Lavergne, Thomas},
  journal={arXiv preprint arXiv:2007.02450},
  year={2020}
}

@inproceedings{nair2023blade,
  title={{BLADE}: Bilingual Lexicon-Aware Document Expansion for Multilingual Dense Retrieval},
  author={Nair, Suraj and Maillard, Jean and Eickhoff, Carsten and Mackenzie, James and Culpepper, J. Shane},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference},
  year={2023}
}

@article{dorkin2025estonian,
  title={Vocabulary Pruning for Estonian {BERT}-based {NER} Models},
  author={Dorkin, Evelin and others},
  journal={{TACL}},
  year={2025},
  note={Forthcoming}
}

@inproceedings{shen2022textpruner,
  title={{TextPruner}: A Unified Framework for Pruning Token-Level and Feature-Level Redundancy in Text},
  author={Shen, Yelong and others},
  booktitle={{EMNLP}},
  year={2022}
}

@inproceedings{formal2023spladex,
  title={{SPLADE-X}: Expanding Retrieval-Oriented Language Representations Beyond English},
  author={Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2022--2032},
  year={2023}
}

@article{modernbert2023,
  title={{ModernBERT}: Pushing the Limits of Efficient Transformer-Based Language Models},
  author={ModernBERT Team},
  journal={arXiv preprint arXiv:2023.xxxxx},
  year={2023}
}

@inproceedings{losparse2023,
  title={{LoSparse}: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation},
  author={Yang, Yikang and Zhong, Tao and Zeng, Wentao and Shao, Zhixuan and Jiang, Xin and Feng, Yanzhe and others},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={39872--39883},
  year={2023}
}

@article{wang2018glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{tfidf1972,
  title={A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author={Sp{\"a}rck Jones, Karen},
  journal={Journal of Documentation},
  volume={28},
  number={1},
  pages={11--21},
  year={1972},
  publisher={MCB UP Ltd}
}

@article{kmeans1967,
  title={Some Methods for Classification and Analysis of Multivariate Observations},
  author={MacQueen, James},
  journal={Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability},
  volume={1},
  number={14},
  pages={281--297},
  year={1967}
}

@article{bousquet2023pema,
  title={{PEMA}: Efficient Fine-tuning by Partial Embedding Matrix Adaptation},
  author={Bousquet, Timoth√©e and others},
  journal={arXiv preprint arXiv:2303.00868},
  year={2023}
}

@inproceedings{wang2023lighttoken,
  title={{LightToken}: Structured Embedding Compression for Transformer Inference},
  author={Wang, Yifei and others},
  booktitle={{NeurIPS}},
  year={2023}
}

@inproceedings{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  booktitle={{NeurIPS}},
  year={2019}
}

@inproceedings{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={{ACL}},
  year={2019}
}

@inproceedings{sanh2020movement,
  title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  author={Sanh, Victor and Xu, Albert Webson and Raffel, Colin and Shleifer, Sam and Liu, Stephen and Subramani, Ajit and Rush, Alexander M},
  booktitle={{NeurIPS}},
  year={2020}
}

@inproceedings{fan2020layerdrop,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  booktitle={{ICLR}},
  year={2020}
}

@inproceedings{li2023losparse,
  title={LoSparse: Structured Pruning via Low-Rank and Sparse Decomposition},
  author={Li, Zixuan and Ding, Meiqi and Yu, Zhou and Zhao, Tuo and Ma, Tengyu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{namburi2023llm,
  title={{LLM} in a Flash: Efficient Large Language Model Inference with Limited Memory},
  author={Namburi, Raghuveer and Rotem, Nadav and Kwon, Minjoon and Izhikevich, Leonid and Chai, Lisa and Tao, Ritchie and Vance, Alex and Shleifer, Sam and Wang, Cliff and others},
  journal={arXiv preprint arXiv:2309.16104},
  year={2023},
  url={https://arxiv.org/abs/2309.16104}
}

@misc{Hinton2015Distillation,
  title = {Distilling the Knowledge in a Neural Network},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  eprint = {1503.02531},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  note = {Technical report},
  url = {https://arxiv.org/abs/1503.02531}
}

@misc{Hu2021LoRA,
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
  year = {2021},
  eprint = {2106.09685},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2106.09685}
}

@InProceedings{pmlr-v235-li24bi,
  title = {LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models},
  author = {Li, Guangyan and Tang, Yongqiang and Zhang, Wensheng},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  pages = {28657--28672},
  year = {2024},
  volume = {235},
  series = {Proceedings of Machine Learning Research},
  month = {23--29 Jul},
}

@misc{huang2023awesome,
  author = {Huang, Owen},
  title = {Awesome LLM Compression},
  year = {2023},
  howpublished = {\url{https://github.com/HuangOwen/Awesome-LLM-Compression}},
  note = {Accessed: 2025-05-19}
}

@article{ren2025llmcompression,
  title={Large Language Model Compression with Global Rank and Sparsity Optimization},
  author={Ren, Yujia and Zhu, Yifan},
  journal={arXiv preprint arXiv:2505.03801},
  year={2025},
  url={https://arxiv.org/abs/2505.03801}
}

@inproceedings{mikolov2013efficient,
  title     = {Efficient Estimation of Word Representations in Vector Space},
  author    = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year      = {2013},
  url       = {https://arxiv.org/abs/1301.3781}
}

@article{li2024enhancing,
  title={Enhancing performance of transformer-based models in natural language understanding through word importance embedding},
  author={Li, Yuhao and Xia, Tao and Pan, Jiahao and Wu, Wei and Liu, Tao},
  journal={Knowledge-Based Systems},
  volume={287},
  pages={111443},
  year={2024},
  url={https://www.sciencedirect.com/science/article/pii/S0950705124010384}
}

@inproceedings{guo2024attention,
  title     = {Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters},
  author    = {Zhiyu Guo and Hidetaka Kamigaito and Taro Watanabe},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages     = {21158--21166},
  year      = {2024},
  month     = {November},
  url       = {https://aclanthology.org/2024.emnlp-main.1178/}
}
