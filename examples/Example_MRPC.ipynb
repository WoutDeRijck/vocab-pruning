{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c904c327-d6c6-470e-b49d-8a57e710e416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vocab-pruning'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 23 (delta 8), reused 23 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (23/23), 33.07 KiB | 1.03 MiB/s, done.\n",
      "Resolving deltas: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/WoutDeRijck/vocab-pruning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293a12f5-34af-4613-a51f-86eeabd55279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from -r vocab-pruning/requirements.txt (line 1)) (2.5.1+cu124)\n",
      "Collecting transformers (from -r vocab-pruning/requirements.txt (line 2))\n",
      "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets (from -r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting scikit-learn (from -r vocab-pruning/requirements.txt (line 4))\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy (from -r vocab-pruning/requirements.txt (line 5))\n",
      "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from -r vocab-pruning/requirements.txt (line 6)) (2.1.2)\n",
      "Collecting pandas (from -r vocab-pruning/requirements.txt (line 7))\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting evaluate (from -r vocab-pruning/requirements.txt (line 8))\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from -r vocab-pruning/requirements.txt (line 9)) (4.66.5)\n",
      "Collecting nltk (from -r vocab-pruning/requirements.txt (line 10))\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting accelerate (from -r vocab-pruning/requirements.txt (line 11))\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->-r vocab-pruning/requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->-r vocab-pruning/requirements.txt (line 1)) (1.3.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers->-r vocab-pruning/requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers->-r vocab-pruning/requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers->-r vocab-pruning/requirements.txt (line 2)) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r vocab-pruning/requirements.txt (line 2))\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers->-r vocab-pruning/requirements.txt (line 2)) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->-r vocab-pruning/requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers->-r vocab-pruning/requirements.txt (line 2))\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp (from datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r vocab-pruning/requirements.txt (line 4))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r vocab-pruning/requirements.txt (line 4))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->-r vocab-pruning/requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->-r vocab-pruning/requirements.txt (line 7)) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas->-r vocab-pruning/requirements.txt (line 7))\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk->-r vocab-pruning/requirements.txt (line 10)) (8.1.7)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->-r vocab-pruning/requirements.txt (line 11)) (6.1.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->-r vocab-pruning/requirements.txt (line 3)) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->-r vocab-pruning/requirements.txt (line 3))\n",
      "  Downloading yarl-1.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r vocab-pruning/requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->-r vocab-pruning/requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->-r vocab-pruning/requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->-r vocab-pruning/requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->-r vocab-pruning/requirements.txt (line 2)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->-r vocab-pruning/requirements.txt (line 1)) (3.0.2)\n",
      "Downloading transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m195.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m239.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m265.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m234.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m160.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m153.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m256.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m157.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading yarl-1.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Installing collected packages: xxhash, tzdata, threadpoolctl, scipy, safetensors, regex, pyarrow, propcache, multidict, joblib, frozenlist, dill, aiohappyeyeballs, yarl, scikit-learn, pandas, nltk, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, accelerate, datasets, evaluate\n",
      "Successfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 frozenlist-1.5.0 huggingface-hub-0.30.2 joblib-1.4.2 multidict-6.4.3 multiprocess-0.70.16 nltk-3.9.1 pandas-2.2.3 propcache-0.3.1 pyarrow-19.0.1 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0 tokenizers-0.21.1 transformers-4.51.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.19.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r vocab-pruning/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d981fd-ec84-4973-bca9-486be171538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-11 11:51:22,232 - INFO - Running with arguments: Namespace(task='mrpc', model_name='answerdotai/ModernBERT-base', pruning_method='frequency', prune_percent=20.0, epochs=15, learning_rate=8e-05, weight_decay=8e-06, batch_size=32, clustering_method='agglomerative', num_clusters=50, importance_type=3, train_ratio=0.8, validation_ratio=0.1, test_ratio=0.1, output_dir='./unified_model_output', train_only=False, seed=42)\n",
      "2025-04-11 11:51:22,496 - INFO - Setting up frequency-based model for mrpc with 20.0% pruning\n",
      "2025-04-11 11:51:22,496 - INFO - Getting dataset vocabulary with counts for mrpc, train_only=True\n",
      "2025-04-11 11:51:28,330 - INFO - Found 14268 unique tokens in the dataset\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-04-11 11:51:29,351 - INFO - Performing frequency-based pruning with prune_percent=20.0%\n",
      "2025-04-11 11:51:30,162 - INFO - Kept 11415 tokens, removed 2857 tokens\n",
      "2025-04-11 11:51:30,163 - INFO - Creating reduced embeddings for task_vocab of size 11415\n",
      "2025-04-11 11:51:30,221 - INFO - Creating custom train/validation/test splits\n",
      "2025-04-11 11:51:30,222 - INFO - Loading mrpc dataset\n",
      "2025-04-11 11:51:33,394 - INFO - Original train set size: 3668\n",
      "2025-04-11 11:51:33,394 - INFO - Original validation set size: 408\n",
      "2025-04-11 11:51:33,397 - INFO - Combined dataset size: 4076\n",
      "2025-04-11 11:51:33,439 - INFO - New train set size: 3260 (80.0%)\n",
      "2025-04-11 11:51:33,439 - INFO - New validation set size: 408 (10.0%)\n",
      "2025-04-11 11:51:33,439 - INFO - New test set size: 408 (10.0%)\n",
      "2025-04-11 11:51:33,455 - INFO - Label distribution:\n",
      "2025-04-11 11:51:33,456 - INFO -   Label 0: original=32.5%, train=32.5%, valid=32.4%, test=32.6%\n",
      "2025-04-11 11:51:33,456 - INFO -   Label 1: original=67.5%, train=67.5%, valid=67.6%, test=67.4%\n",
      "2025-04-11 11:51:33,534 - INFO - Applying vocabulary mapping to custom splits\n",
      "Remapping token IDs: 100%|█████████| 3260/3260 [00:00<00:00, 6767.25 examples/s]\n",
      "Remapping token IDs: 100%|███████████| 408/408 [00:00<00:00, 6371.26 examples/s]\n",
      "Remapping token IDs: 100%|███████████| 408/408 [00:00<00:00, 6449.83 examples/s]\n",
      "2025-04-11 11:51:34,267 - INFO - Created custom splits with sizes: train=3260, validation=408, test=408\n",
      "2025-04-11 11:51:34,437 - INFO - \n",
      "=== Vocabulary Statistics ===\n",
      "2025-04-11 11:51:34,457 - INFO - Original vocabulary size: 50368\n",
      "2025-04-11 11:51:34,457 - INFO - Kept tokens: 11415\n",
      "2025-04-11 11:51:34,474 - INFO - Vocabulary reduction: 77.34%\n",
      "2025-04-11 11:51:34,475 - INFO - Using custom splits with ratios: train=0.8, val=0.1, test=0.1\n",
      "2025-04-11 11:51:34,475 - INFO - Starting training for 15 epochs\n",
      "{'loss': 0.5681, 'grad_norm': 7.455329895019531, 'learning_rate': 7.471895424836602e-05, 'epoch': 1.0}\n",
      "  7%|██▋                                     | 102/1530 [00:07<01:36, 14.78it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 47.05it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5716466307640076, 'eval_accuracy': 0.7475490196078431, 'eval_f1': 0.790224032586558, 'eval_runtime': 1.0511, 'eval_samples_per_second': 388.176, 'eval_steps_per_second': 12.368, 'epoch': 1.0}\n",
      "  7%|██▋                                     | 102/1530 [00:08<01:36, 14.78it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00, 41.25it/s]\u001b[A\n",
      "{'loss': 0.2968, 'grad_norm': 5.618460655212402, 'learning_rate': 6.938562091503269e-05, 'epoch': 2.0}\n",
      " 13%|█████▎                                  | 204/1530 [00:20<01:34, 14.05it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 47.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.47929906845092773, 'eval_accuracy': 0.8088235294117647, 'eval_f1': 0.8645833333333334, 'eval_runtime': 1.0157, 'eval_samples_per_second': 401.686, 'eval_steps_per_second': 12.799, 'epoch': 2.0}\n",
      " 13%|█████▎                                  | 204/1530 [00:21<01:34, 14.05it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 41.35it/s]\u001b[A\n",
      "{'loss': 0.1224, 'grad_norm': 3.662743091583252, 'learning_rate': 6.405228758169936e-05, 'epoch': 3.0}\n",
      " 20%|████████                                | 306/1530 [00:31<01:27, 14.03it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 47.19it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5870122909545898, 'eval_accuracy': 0.8235294117647058, 'eval_f1': 0.8656716417910447, 'eval_runtime': 1.003, 'eval_samples_per_second': 406.769, 'eval_steps_per_second': 12.961, 'epoch': 3.0}\n",
      " 20%|████████                                | 306/1530 [00:32<01:27, 14.03it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 41.08it/s]\u001b[A\n",
      "{'loss': 0.0585, 'grad_norm': 19.613597869873047, 'learning_rate': 5.871895424836601e-05, 'epoch': 4.0}\n",
      " 27%|██████████▋                             | 408/1530 [00:43<01:21, 13.70it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.42it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.99884033203125, 'eval_accuracy': 0.8406862745098039, 'eval_f1': 0.8837209302325582, 'eval_runtime': 1.0428, 'eval_samples_per_second': 391.248, 'eval_steps_per_second': 12.466, 'epoch': 4.0}\n",
      " 27%|██████████▋                             | 408/1530 [00:44<01:21, 13.70it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00, 40.62it/s]\u001b[A\n",
      "{'loss': 0.0257, 'grad_norm': 0.08921550959348679, 'learning_rate': 5.338562091503268e-05, 'epoch': 5.0}\n",
      " 33%|█████████████▎                          | 510/1530 [00:56<01:11, 14.27it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.431182861328125, 'eval_accuracy': 0.7867647058823529, 'eval_f1': 0.8277227722772277, 'eval_runtime': 1.0988, 'eval_samples_per_second': 371.318, 'eval_steps_per_second': 11.831, 'epoch': 5.0}\n",
      " 33%|█████████████▎                          | 510/1530 [00:57<01:11, 14.27it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00, 39.94it/s]\u001b[A\n",
      "{'loss': 0.0326, 'grad_norm': 0.10190449655056, 'learning_rate': 4.8052287581699346e-05, 'epoch': 6.0}\n",
      " 40%|████████████████                        | 612/1530 [01:07<01:07, 13.55it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.61it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3086624145507812, 'eval_accuracy': 0.7990196078431373, 'eval_f1': 0.8416988416988417, 'eval_runtime': 1.0612, 'eval_samples_per_second': 384.48, 'eval_steps_per_second': 12.251, 'epoch': 6.0}\n",
      " 40%|████████████████                        | 612/1530 [01:08<01:07, 13.55it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00, 40.65it/s]\u001b[A\n",
      "{'loss': 0.0144, 'grad_norm': 24.40216636657715, 'learning_rate': 4.2718954248366014e-05, 'epoch': 7.0}\n",
      " 47%|██████████████████▋                     | 714/1530 [01:19<00:56, 14.54it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.44it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5584195852279663, 'eval_accuracy': 0.7965686274509803, 'eval_f1': 0.8349900596421471, 'eval_runtime': 1.013, 'eval_samples_per_second': 402.781, 'eval_steps_per_second': 12.834, 'epoch': 7.0}\n",
      " 47%|██████████████████▋                     | 714/1530 [01:20<00:56, 14.54it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 40.77it/s]\u001b[A\n",
      "{'loss': 0.023, 'grad_norm': 0.012001993134617805, 'learning_rate': 3.738562091503268e-05, 'epoch': 8.0}\n",
      " 53%|█████████████████████▎                  | 816/1530 [01:30<00:51, 13.77it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.63it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.325441598892212, 'eval_accuracy': 0.8382352941176471, 'eval_f1': 0.8759398496240601, 'eval_runtime': 1.0367, 'eval_samples_per_second': 393.543, 'eval_steps_per_second': 12.539, 'epoch': 8.0}\n",
      " 53%|█████████████████████▎                  | 816/1530 [01:31<00:51, 13.77it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00, 40.40it/s]\u001b[A\n",
      "{'loss': 0.0058, 'grad_norm': 0.007780902087688446, 'learning_rate': 3.205228758169935e-05, 'epoch': 9.0}\n",
      " 60%|████████████████████████                | 918/1530 [01:41<00:42, 14.42it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.82it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.9053632020950317, 'eval_accuracy': 0.8431372549019608, 'eval_f1': 0.8888888888888888, 'eval_runtime': 1.0497, 'eval_samples_per_second': 388.696, 'eval_steps_per_second': 12.385, 'epoch': 9.0}\n",
      " 60%|████████████████████████                | 918/1530 [01:42<00:42, 14.42it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00, 40.96it/s]\u001b[A\n",
      "{'loss': 0.0057, 'grad_norm': 0.0007024761289358139, 'learning_rate': 2.6718954248366015e-05, 'epoch': 10.0}\n",
      " 67%|██████████████████████████             | 1020/1530 [01:52<00:36, 13.84it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.52it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7417240142822266, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.8697318007662835, 'eval_runtime': 1.0011, 'eval_samples_per_second': 407.55, 'eval_steps_per_second': 12.986, 'epoch': 10.0}\n",
      " 67%|██████████████████████████             | 1020/1530 [01:53<00:36, 13.84it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 39.78it/s]\u001b[A\n",
      "{'loss': 0.0041, 'grad_norm': 0.09895765781402588, 'learning_rate': 2.138562091503268e-05, 'epoch': 11.0}\n",
      " 73%|████████████████████████████▌          | 1122/1530 [02:04<00:28, 14.29it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.70it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8327065706253052, 'eval_accuracy': 0.8259803921568627, 'eval_f1': 0.8662900188323918, 'eval_runtime': 1.2075, 'eval_samples_per_second': 337.883, 'eval_steps_per_second': 10.766, 'epoch': 11.0}\n",
      " 73%|████████████████████████████▌          | 1122/1530 [02:05<00:28, 14.29it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00, 40.63it/s]\u001b[A\n",
      "{'loss': 0.0012, 'grad_norm': 0.00012977476580999792, 'learning_rate': 1.6052287581699348e-05, 'epoch': 12.0}\n",
      " 80%|███████████████████████████████▏       | 1224/1530 [02:15<00:20, 14.66it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.48it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.2700579166412354, 'eval_accuracy': 0.8406862745098039, 'eval_f1': 0.8911222780569514, 'eval_runtime': 1.0114, 'eval_samples_per_second': 403.413, 'eval_steps_per_second': 12.854, 'epoch': 12.0}\n",
      " 80%|███████████████████████████████▏       | 1224/1530 [02:16<00:20, 14.66it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 40.54it/s]\u001b[A\n",
      "{'loss': 0.005, 'grad_norm': 0.00017444750119466335, 'learning_rate': 1.0718954248366014e-05, 'epoch': 13.0}\n",
      " 87%|█████████████████████████████████▊     | 1326/1530 [02:27<00:14, 14.35it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.58it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.9414831399917603, 'eval_accuracy': 0.8259803921568627, 'eval_f1': 0.870201096892139, 'eval_runtime': 1.0245, 'eval_samples_per_second': 398.243, 'eval_steps_per_second': 12.689, 'epoch': 13.0}\n",
      " 87%|█████████████████████████████████▊     | 1326/1530 [02:28<00:14, 14.35it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 40.82it/s]\u001b[A\n",
      "{'loss': 0.0001, 'grad_norm': 0.00024600178585387766, 'learning_rate': 5.38562091503268e-06, 'epoch': 14.0}\n",
      " 93%|████████████████████████████████████▍  | 1428/1530 [02:38<00:07, 13.74it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 46.64it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.948843240737915, 'eval_accuracy': 0.8431372549019608, 'eval_f1': 0.8853046594982079, 'eval_runtime': 1.288, 'eval_samples_per_second': 316.78, 'eval_steps_per_second': 10.093, 'epoch': 14.0}\n",
      " 93%|████████████████████████████████████▍  | 1428/1530 [02:39<00:07, 13.74it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00, 40.60it/s]\u001b[A\n",
      "{'loss': 0.0, 'grad_norm': 0.00030113145476207137, 'learning_rate': 5.228758169934641e-08, 'epoch': 15.0}\n",
      "100%|███████████████████████████████████████| 1530/1530 [02:50<00:00, 14.20it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:00<00:00, 45.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.026869773864746, 'eval_accuracy': 0.8455882352941176, 'eval_f1': 0.8884955752212389, 'eval_runtime': 1.0607, 'eval_samples_per_second': 384.64, 'eval_steps_per_second': 12.256, 'epoch': 15.0}\n",
      "100%|███████████████████████████████████████| 1530/1530 [02:51<00:00, 14.20it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:01<00:00, 40.24it/s]\u001b[A\n",
      "{'train_runtime': 173.3167, 'train_samples_per_second': 282.142, 'train_steps_per_second': 8.828, 'train_loss': 0.07756115323719355, 'epoch': 15.0}\n",
      "100%|███████████████████████████████████████| 1530/1530 [02:53<00:00,  8.83it/s]\n",
      "2025-04-11 11:54:28,314 - INFO - Running final evaluation\n",
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 13.38it/s]\n",
      "2025-04-11 11:54:29,328 - INFO - eval_loss: 0.4814\n",
      "2025-04-11 11:54:29,328 - INFO - eval_accuracy: 0.8064\n",
      "2025-04-11 11:54:29,328 - INFO - eval_f1: 0.8631\n",
      "2025-04-11 11:54:29,328 - INFO - eval_runtime: 1.0116\n",
      "2025-04-11 11:54:29,328 - INFO - eval_samples_per_second: 403.3250\n",
      "2025-04-11 11:54:29,328 - INFO - eval_steps_per_second: 12.8510\n",
      "2025-04-11 11:54:29,328 - INFO - epoch: 15.0000\n",
      "2025-04-11 11:54:29,333 - INFO - Saved training results to ./unified_model_output/mrpc_frequency_prune20.0_results.csv\n",
      "2025-04-11 11:54:29,334 - INFO - \n",
      "Analyzing original model for comparison\n",
      "2025-04-11 11:54:29,620 - INFO - \n",
      "=== Parameter Reduction Statistics ===\n",
      "2025-04-11 11:54:29,620 - INFO - Total parameter reduction: 20.02%\n",
      "2025-04-11 11:54:29,621 - INFO - Embedding parameter reduction: 77.34%\n",
      "2025-04-11 11:54:29,621 - INFO - Model_Only parameter reduction: 0.04%\n",
      "2025-04-11 11:54:29,621 - INFO - \n",
      "=== Summary Results ===\n",
      "2025-04-11 11:54:29,621 - INFO - Task: mrpc\n",
      "2025-04-11 11:54:29,621 - INFO - Pruning method: frequency\n",
      "2025-04-11 11:54:29,621 - INFO - Prune percent: 20.0%\n",
      "2025-04-11 11:54:29,643 - INFO - Vocabulary reduction: 77.34%\n",
      "2025-04-11 11:54:29,643 - INFO - Final loss: 0.4814\n",
      "2025-04-11 11:54:29,644 - INFO - Final accuracy: 0.8064\n",
      "2025-04-11 11:54:29,644 - INFO - Final f1: 0.8631\n",
      "2025-04-11 11:54:29,644 - INFO - Final runtime: 1.0116\n",
      "2025-04-11 11:54:29,644 - INFO - Final samples_per_second: 403.3250\n",
      "2025-04-11 11:54:29,644 - INFO - Final steps_per_second: 12.8510\n",
      "2025-04-11 11:54:29,644 - INFO - \n",
      "=== Test Set Prediction Generation ===\n",
      "2025-04-11 11:54:29,644 - INFO - Generating predictions on test set\n",
      "Predicting: 100%|███████████████████████████████| 13/13 [00:00<00:00, 37.70it/s]\n",
      "2025-04-11 11:54:29,990 - INFO - Saved test predictions to ./unified_model_output/mrpc_frequency_prune20.0_predictions.txt\n",
      "2025-04-11 11:54:29,991 - INFO - \n",
      "=== Test Set Evaluation Results ===\n",
      "2025-04-11 11:54:29,992 - INFO - accuracy_score: 0.8480\n",
      "2025-04-11 11:54:29,993 - INFO - f1_score: 0.8916\n",
      "2025-04-11 11:54:30,007 - INFO - Pipeline completed successfully\n"
     ]
    }
   ],
   "source": [
    "!python vocab-pruning/unified_pruning.py \\\n",
    "    --task mrpc \\\n",
    "    --pruning_method frequency \\\n",
    "    --prune_percent 20 \\\n",
    "    --epochs 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70a9da-7b62-43c1-8648-89326490e5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
